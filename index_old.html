<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="utf-8">
<title>演習ファイル：リアルタイム指文字認識</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
<style>
body{font-family:system-ui,sans-serif;background:linear-gradient(135deg, #667eea 0%, #764ba2 100%);color:white;text-align:center;margin:0;padding:1rem;min-height:100vh;}
.container{max-width:800px;margin:0 auto;background:rgba(255,255,255,0.1);border-radius:20px;padding:2rem;backdrop-filter:blur(10px);}
canvas{border:2px solid rgba(255,255,255,0.3);border-radius:15px;max-width:100%;height:auto;box-shadow:0 8px 32px rgba(0,0,0,0.3);}
button,input{margin:0.5rem;padding:0.8rem 1.5rem;font-size:1rem;border:none;border-radius:10px;cursor:pointer;transition:all 0.3s ease;}
button{background:linear-gradient(45deg, #ff6b6b, #ee5a24);color:white;font-weight:bold;}
button:hover:not(:disabled){transform:translateY(-2px);box-shadow:0 5px 15px rgba(0,0,0,0.3);}
button:disabled{background:#ccc;cursor:not-allowed;transform:none;}
input[type="file"]{background:rgba(255,255,255,0.2);color:white;padding:0.5rem;}
#prob{font-weight:bold;font-size:2rem;color:#ffd700;text-shadow:2px 2px 4px rgba(0,0,0,0.5);margin:1rem 0;padding:1rem;background:rgba(0,0,0,0.2);border-radius:15px;}
#status{font-size:1rem;color:#a8e6cf;margin:0.5rem;}
h2{margin:1rem 0;font-size:2.5rem;text-shadow:2px 2px 4px rgba(0,0,0,0.5);}
p{margin:1rem 0;font-size:1.1rem;opacity:0.9;}
.info-panel{background:rgba(255,255,255,0.1);border-radius:10px;padding:1rem;margin:1rem 0;text-align:left;}
</style>
</head>
<body>
<div class="container">
<h2>🤟 リアルタイム指文字認識システム</h2>
<p>数字 1-9 の指文字を認識します。左手・右手どちらでも対応！</p>

<div class="info-panel">
  <h3>📋 使い方</h3>
  <ol style="text-align:left;">
    <li>下のボタンからモデルファイル（JSON）を選択</li>
    <li>「認識開始」ボタンをクリック</li>
    <li>カメラに向かって指文字を表示</li>
    <li>認識結果が下に表示されます</li>
  </ol>
</div>

<label style="display:block;margin:1rem 0;">
  🤖 モデルファイル (JSON): 
  <input type="file" id="modelFile" accept=".json">
</label>

<div style="margin:1rem 0;">
  <button id="start">🚀 認識開始</button>
  <button id="stop" disabled>⏹️ 停止</button>
</div>

<div id="status">モデルを読み込んでください</div>
<div id="prob">–</div>

<video id="input" style="display:none;"></video>
<canvas id="output" width="640" height="480"></canvas>

<div class="info-panel" style="margin-top:2rem;">
  <h3>💡 認識のコツ</h3>
  <ul style="text-align:left;">
    <li>手をカメラの中央に配置してください</li>
    <li>指の形をはっきりと見せてください</li>
    <li>背景はシンプルなものが良いです</li>
    <li>適度な明るさの環境で使用してください</li>
  </ul>
</div>
</div>

<script>
// --- このファイルは、学生が編集しやすいように主要な関数や変数のみを記載しています ---

const video = document.getElementById('input');
const canvas = document.getElementById('output');
const ctx = canvas.getContext('2d');
const startBtn = document.getElementById('start');
const stopBtn  = document.getElementById('stop');
const probDiv  = document.getElementById('prob');
const statusDiv = document.getElementById('status');
let modelCoef, modelIntercept, classLabels, scMean, scScale;
let modelLoaded = false, sending = false; 

// --- モデル読み込み、softmax, predict関数 (この部分は完成形で提供) ---
document.getElementById('modelFile').addEventListener('change', e=>{
  const file = e.target.files[0]; if(!file) return;
  statusDiv.textContent = 'モデルを読み込み中...';
  file.text().then(txt=>{
    const obj = JSON.parse(txt);
    modelCoef = obj.coef; modelIntercept = obj.intercept; classLabels = obj.labels;
    scMean  = obj.mean; scScale = obj.scale; modelLoaded = true;
    startBtn.disabled = false; 
    statusDiv.textContent = '✅ モデル読み込み完了！認識を開始できます';
    alert('🎉 モデルを読み込みました！');
  }).catch(err => {
    statusDiv.textContent = '❌ モデルの読み込みに失敗しました';
    console.error('Model loading error:', err);
  });
});
function softmax(arr){
  const ex = arr.map(Math.exp); const sum = ex.reduce((a,b)=>a+b); return ex.map(v=>v/sum);
}
function predict(vec){
  const scores = modelCoef.map((w,i)=>{
    let s = modelIntercept[i];
    for(let j=0;j<w.length;j++){ s += w[j]*vec[j]; } return s;
  });
  const probs = softmax(scores); const maxIdx = probs.indexOf(Math.max(...probs));
  return [classLabels[maxIdx], probs[maxIdx]];
}

// --- Mediapipe & カメラのセットアップ (この部分も完成形で提供) ---
const hands = new Hands({locateFile:file=>`https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`});
hands.setOptions({maxNumHands:1,modelComplexity:1,minDetectionConfidence:0.5,minTrackingConfidence:0.5});
const camera = new Camera(video,{
  width:640,height:480,
  onFrame: async ()=>{ if(sending) return; sending = true; await hands.send({image: video}); sending = false; }
});

/**
 * Mediapipeが手のランドマークを検出したときに呼び出されるメインの処理
 */
hands.onResults(res => {
  ctx.clearRect(0,0,canvas.width,canvas.height);
  ctx.drawImage(res.image,0,0,canvas.width,canvas.height);
  if (!modelLoaded || !res.multiHandLandmarks?.length) { probDiv.textContent = '–'; return; }

  /* ---------- 1. 21点ランドマーク取得 (ここは完成形で提供) ---------- */
  const lmRaw = res.multiHandLandmarks[0];
  let lm = lmRaw.map(p => [p.x, p.y]);

  // 左手対応：左手の場合はx座標を反転して右手として扱う
  const handedness = res.multiHandedness?.[0]?.label;
  if (handedness === 'Left') {
    lm = lm.map(point => [1 - point[0], point[1]]);
  }


  /*
   * =================================================================
   * ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
   * 【課題】ここから下のデータ前処理を実装しよう！
   * Pythonのノートブック(exp3_compare.ipynb)の `create_normalized_features` 関数を参考にしてください。
   * ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
   */

  /* ---------- 2. 手首を原点に移動（位置の正規化）---------- */
  // 目的: 手が画面上のどこにあっても同じ特徴が得られるようにするため
  // Pythonコード: lm -= wrist
  const wrist = [...lm[0]]; // 重要なコピー処理は予め記述しておきます

  // JavaScriptらしい実装：mapを使用
  lm.forEach((point, i) => {
    lm[i] = [point[0] - wrist[0], point[1] - wrist[1]];
  });

  /* ---------- 3. スケール正規化（大きさの正規化）---------- */
  // 目的: 手の大きさ（カメラとの距離）が違っても同じ特徴が得られるようにするため
  // Pythonコード: norm = np.linalg.norm(lm, axis=1).max(); lm /= norm

  // ステップA: 各点と原点(手首)との距離を計算する（JavaScriptらしい実装）
  const distances = lm.map(point => Math.hypot(point[0], point[1]));
  
  // ステップB: 距離の最大値を見つける（ここは完成形で提供）
  const norm = Math.max(...distances) || 1; // 距離が0の時のエラーを防ぐため || 1 を付けます

  // ステップC: 全ての座標を、距離の最大値(norm)で割る（JavaScriptらしい実装）
  lm.forEach((point, i) => {
    lm[i] = [point[0] / norm, point[1] / norm];
  });

  /* * ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
   * 実装はここまで！
   * ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲
   */


  /* ---------- 4. ベクトル化、標準化、予測 (ここは完成形で提供) ---------- */
  const vec = lm.flat();
  const stdVec = vec.map((v,i) => (v - scMean[i]) / scScale[i]);
  const [pred, prob] = predict(stdVec);
  probDiv.textContent = `数字: ${pred}  (確信度 ${prob.toFixed(2)})`;

  /* ---------- 5. 描画 (ここは完成形で提供) ---------- */
  drawConnectors(ctx, lmRaw, HAND_CONNECTIONS, {color:'#22c55e', lineWidth:3});
  drawLandmarks(ctx, lmRaw, {color:'#ef4444', radius:3});
});

// --- UI操作関連 (この部分も完成形で提供) ---
startBtn.disabled = true;
startBtn.onclick = ()=>{ 
  if(!modelLoaded){ alert("先にモデルを読み込んで下さい"); return; }
  camera.start(); startBtn.disabled=true; stopBtn.disabled=false;
};
stopBtn.onclick = ()=>{
  camera.stop(); startBtn.disabled = false; stopBtn.disabled = true; probDiv.textContent = '–';
};
</script>
</body>
</html>